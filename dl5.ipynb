{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69cc447-6dae-4c2a-9f1f-b1bbe4d6257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ridership = pd.read_csv(\"./../Data/CTA-Ridership-Daily_Boarding_Totals_20240829.csv\", parse_dates=[\"service_date\"])\n",
    "\n",
    "\n",
    "display(ridership)\n",
    "\n",
    "# Setting date column as index\n",
    "ridership = ridership.sort_values(\"service_date\").set_index(\"service_date\")\n",
    "\n",
    "# Checks for the modification into the dataset\n",
    "display(ridership)\n",
    "\n",
    "# Drops the calculated column \"total_rides\" as this is just element-wise addition from columns \"bus\" and \"rail_boardings\".\n",
    "ridership = ridership.drop(\"total_rides\", axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# remove duplicate observations, if any\n",
    "ridership = ridership.drop_duplicates()\n",
    "\n",
    "\n",
    "ridership.shape\n",
    "\n",
    "\n",
    "\n",
    "# Looks at the first few months of 2019\n",
    "ridership[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Naive Forecasting\n",
    "# Creates a 7-day differencing time-series out of difference between two time-series - the original one\n",
    "# and other one lagged by one week\n",
    "diff_7 = ridership[[\"bus\", \"rail_boardings\"]].diff(7)[\"2019-03\":\"2019-05\"]\n",
    "\n",
    "\n",
    "# First shows the two overlayed time-series\n",
    "\n",
    "# Prepares a figure with two plots - one for the overlayed time-series and \n",
    "# other is for the differencing time-series\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\n",
    "\n",
    "# Plots the original time-series in the first axis\n",
    "ridership.plot(ax=axs[0], legend=False, marker=\".\")  \n",
    "\n",
    "# Plots the lagged time-series in the first axis as overlayed\n",
    "ridership.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")\n",
    "\n",
    "# Plots the differencing time-series prepared earlier in the next axis\n",
    "diff_7.plot(ax=axs[1], grid=True, marker=\".\")\n",
    "\n",
    "# [OPTIONAL] Sets y-axis limit of the first plot to zoom into range of interest\n",
    "axs[0].set_ylim([170_000, 900_000])\n",
    "\n",
    "# Shows the prepared plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Measures the performance of naive forecasting over metric Mean absolute error (MAE), also called mean absolute deviation (MAD)\n",
    "diff_7.abs().mean()\n",
    "\n",
    "\n",
    "# The same performance is also expressed in Mean Absolute Percentage Error (MAPE)\n",
    "(diff_7 / ridership[[\"bus\", \"rail_boardings\"]][\"2019-03\":\"2019-05\"]).abs().mean()\n",
    "\n",
    "The naive forecasts give a MAPE of roughly **8.3%** for bus and **9.0%** for rail boardings.\n",
    "\n",
    "\n",
    "### Univariate Forecasting\n",
    "# Splits the time-series into three periods, for training, validation and testing\n",
    "# The values are scaled down by a factor of one million, to ensure the values are near the 0â€“1 range\n",
    "rail_train = ridership[\"rail_boardings\"][\"2016-01\":\"2018-12\"] / 1e6  # 3 years\n",
    "rail_val = ridership[\"rail_boardings\"][\"2019-01\":\"2019-05\"] / 1e6    # 5 months\n",
    "rail_test = ridership[\"rail_boardings\"][\"2019-06\":] / 1e6            # remaining period from 2019-06\n",
    "\n",
    "\n",
    "# Prepares TensorFlow specific datasets\n",
    "\n",
    "seq_length = 56    # represents sequence of past 8 weeks (56 days) of ridership data\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rail_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    rail_train.to_numpy(),\n",
    "    targets=rail_train[seq_length:],\n",
    "    sequence_length=seq_length,\n",
    "    batch_size=32,\n",
    "    shuffle=True,  # shuffling in training windows is recommended for gradient descent optimizer\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rail_val_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    rail_val.to_numpy(),\n",
    "    targets=rail_val[seq_length:],\n",
    "    sequence_length=seq_length,\n",
    "    batch_size=32,\n",
    "    shuffle=False  # shuffling is not required for any testing data including validation data\n",
    ")\n",
    "\n",
    "\n",
    "#### Forecasting using A Linear Model\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creates a linear model over a dense layer\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[seq_length])])\n",
    "\n",
    "# Sets callback to stop training when model does improve after a certain number of training iterations\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# Sets the model optimizer and compiles it with specific loss function and metric\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),   # Huber loss works well in minimizing MAE\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9), \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "# Starts model training process over specified training, validation data and callbacks\n",
    "history = model.fit(rail_train_ds, \n",
    "                    validation_data=rail_val_ds, \n",
    "                    epochs=500,\n",
    "                    callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "#### Forecasting Using a Simple RNN\n",
    "\n",
    "# Resets all the keras states\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creates an RNN with 32 recurrent neurons followed by a dense output layer with one output neuron\n",
    "univar_simple_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Sets callback to stop training when model does improve after a certain number of training iterations\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# Sets the model optimizer and compiles it with specific loss function and metric\n",
    "univar_simple_rnn.compile(\n",
    "    loss=\"huber\",\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9),\n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "# Starts model training process over specified training, validation data and callbacks\n",
    "history = univar_simple_rnn.fit(\n",
    "    rail_train_ds, validation_data=rail_val_ds, epochs=500, callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After training, model gets evaluated against validation data\n",
    "\n",
    "val_loss, val_mae = univar_simple_rnn.evaluate(rail_val_ds)\n",
    "print(\"Validation MAE of the Simple RNN:\", val_mae * 1e6)\n",
    "\n",
    "\n",
    "The model performance shown above indicates that the Simple RNN worked [MAE: 29935] better than the baseline model [MAE: 42143] and the linear model [MAE: 37060].\n",
    "\n",
    "\n",
    "\n",
    "#### Forecasting Using a Deep RNN\n",
    "\n",
    "# Resets all the keras states\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creates a Deep RNN with multiple layers of simple RNN each with 32 recurrent neurons \n",
    "# followed by a dense output layer with one output neuron\n",
    "univar_deep_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),  # sequence-to-sequence layer\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True),                         # sequence-to-sequence layer\n",
    "    tf.keras.layers.SimpleRNN(32),                                                # sequence-to-vector layer\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "# Sets callback to stop training when model does improve after a certain number of training iterations\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
    "\n",
    "# Sets the model optimizer and compiles it specific loss function and metric\n",
    "univar_deep_rnn.compile(\n",
    "    loss=\"huber\",\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "# Starts model training process over specified training, validation data and callbacks\n",
    "history = univar_deep_rnn.fit(\n",
    "    rail_train_ds, validation_data=rail_val_ds, epochs=500, callbacks=[early_stopping_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After training, model gets evaluated against validation data\n",
    "\n",
    "val_loss, val_mae = univar_deep_rnn.evaluate(rail_val_ds)\n",
    "print(\"Validation MAE of the Deep RNN:\", val_mae * 1e6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The above performance shows that deep RNN is better than baseline model [MAE: 42143] and the linear model [MAE: 37060], but it couldn't beat simple RNN [MAE: 29935]. Hence, deep RNN might not be a good fit for this case. Now, let's experiment with multivariate forecasting to check if it can improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
